---
title: "PS4: Spatial"
author: "Hiroaki Kurachi, Luis Senires"
date: today
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 

## Style Points (10 pts)

## Submission Steps (10 pts)

1. This problem set is a paired problem set.

2. Play paper, scissors, rock to determine who goes first. Call that person Partner 1.

  • Partner 1 (Hiroaki Kurachi and hiroakik):

  • Partner 2 (Luis Senires and ldsenires):

3. Partner 1 will accept the ps4 and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted.

4. “This submission is our work alone and complies with the 30538 integrity policy.” Add your initials to indicate your agreement: **HK** **__**

5. “I have uploaded the names of anyone else other than my partner and I worked with on the problem set here” (1 point)

6. Late coins used this pset: **__** Late coins left after submission: **__**

7. Knit your ps4.qmd to an PDF file to make ps4.pdf,

  • The PDF should not be more than 25 pages. Use head() and re-size figures when appropriate.

8. (Partner 1): push ps4.qmd and ps4.pdf to your github repo.

9. (Partner 1): submit ps4.pdf via Gradescope. Add your partner on Gradescope.

10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers.

```{python}
import os
import pandas as pd
import altair as alt
import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
import time
import datetime as datetime

import warnings
warnings.filterwarnings('ignore')
```

## Download and explore the Provider of Services (POS) file (10 pts)

1. 

The variables we selected are;

- PRVDR_NUM: CMS certification number

- PRVDR_CTGRY_CD: the type of provider participating in the Medicare/Medicaid program

- PRVDR_CTGRY_SBTYP_CD: the subtype of the provider, within the primary category

- FAC_NAME: name of the provider certified to participate in the Medicare and/or Medicaid programs

- CITY_NAME: city in which the provider is physically located

- ST_ADR: street address where the provider is located

- ZIP_CD; five-digit ZIP code for a provider's physical address

- TRMNTN_EXPRTN_DT: date the provider was terminated

- PGM_TRMNTN_CD: indicates the current termination status for the provider

2. 

```{python}
# Set the absolute path of the data folder
base = (r"C:\Users\hkura\Documents\Uchicago\04 2024 Autumn\Python2\problem-set-4-hiro-luis\data")
# base = (r"C:\Users\LUIS\Documents\GitHub\problem-set-4-hiro-luis\data")
path_data_2016 = os.path.join(base, "pos2016.csv")

# Load 2016 dataset
df_pos2016 = pd.read_csv(path_data_2016, encoding="utf-8")

# Define function to subset a dataset for short-term hospitals


def subset_short_term(df):
    df_result = df[(df["PRVDR_CTGRY_CD"] == 1) &
                   (df["PRVDR_CTGRY_SBTYP_CD"] == 1)]
    return df_result


# Create short-term hospitals subsets for 2016 dataset
df_pos2016_short = subset_short_term(df_pos2016)
```

    a.    
```{python}
# Count the number of observations
print(len(df_pos2016_short))
```

    This subset for short-term hospitals in 2016 includes 7245 hospitals.

    This number seems not making sense, as it occupies a half of number of whole hospitals though it is just one subtype. 

```{python}
# Count the number of observations
print(len(df_pos2016))
```

    b. We couldn't find references which includes the exact number of short-term hospitals at the end of 2016, but the 2016 CMS statistics specifies the number as 3,436 for the Dec 31, 2015, which is the most recent data available in the search result(https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/CMS-Statistics-Reference-Booklet/Downloads/2016_CMS_Stats.pdf).

    Considering the number 3,436 at the end of 2015, 7245 is drastically larger, even considering the change in one year.

    As far as we could find, there seems no substantial policy change in 2016, which could potentially change the number of healthcare providers drastically.

    As both the dataset we use and the cross-reference are both published by Centers for Medicare & Medicaid Services, it might be plausible to assume their credibilities. And as the dataset dictionary says nothing about change in definition of the provider categories/sub-categories. Then one reason we can consider is, for instance, the change in the data gathering methods. 

3. 

```{python}
# Load 2017-2019 datasets
path_data_2017 = os.path.join(base, "pos2017.csv")
path_data_2018 = os.path.join(base, "pos2018.csv")
path_data_2019 = os.path.join(base, "pos2019.csv")
df_pos2017 = pd.read_csv(path_data_2017, encoding="utf-8")
df_pos2018 = pd.read_csv(path_data_2018, encoding="latin1")
df_pos2019 = pd.read_csv(path_data_2019, encoding="latin1")

# Clean 2018, 2019 datasets: fix colname


def fix_col(df):
    col = df.columns.tolist()
    col[0] = "PRVDR_NUM"
    df.columns = col
    return df


df_pos2018 = fix_col(df_pos2018)
df_pos2019 = fix_col(df_pos2019)

# Create short-term hospitals subsets for 2017-2019 datasets
df_pos2017_short = subset_short_term(df_pos2017)
df_pos2018_short = subset_short_term(df_pos2018)
df_pos2019_short = subset_short_term(df_pos2019)


# Summarize the number of observations for each subset
df_summary_obs = pd.DataFrame(
  {"year": list(range(2016, 2020, 1)), 
  "observations" : [len(df_pos2016_short), len(df_pos2017_short), len(df_pos2018_short), len(df_pos2019_short)]
  }
)

df_summary_obs
```

  The number for q4 in 2017/2018/2019 is on similar level with 2016, which are unmatched with the cross-reference.

```{python}
# Add "DATA_YEAR" column to each subsets
df_pos2016_short["DATA_YEAR"] = 2016
df_pos2017_short["DATA_YEAR"] = 2017
df_pos2018_short["DATA_YEAR"] = 2018
df_pos2019_short["DATA_YEAR"] = 2019

# Concaterate the subsets
df_pos_append = pd.concat(
    [df_pos2016_short, df_pos2017_short, df_pos2018_short, df_pos2019_short])

# Turn off the row number limit
alt.data_transformers.disable_max_rows()

# Plot the number of observation for each subset's year
chart_obs = alt.Chart(df_pos_append).mark_line().encode(
    alt.X("DATA_YEAR:N", title = "Year (Q4)", axis = alt.Axis(labelAngle = 0)),
    alt.Y("count()", title = "Number of Observation", scale = alt.Scale(domain = [7000,8000]))
).properties(width = 250, height = 250)
chart_obs
```

4. 
    a.

```{python}
# Summarize the number of unique providers for each subset
df_summary_unique = pd.DataFrame(
  {"year": list(range(2016, 2020, 1)), 
  "unique_hospitals" :  [len(df_pos2016_short["PRVDR_NUM"].value_counts()), len(df_pos2017_short["PRVDR_NUM"].value_counts()), len(df_pos2018_short["PRVDR_NUM"].value_counts()), len(df_pos2019_short["PRVDR_NUM"].value_counts())]
  }
)

df_summary_unique

# Plot the number of unique hospitals for each subset's year
chart_unique = alt.Chart(df_summary_unique).mark_line().encode(
    alt.X("year:N", title = "Year (Q4)", axis = alt.Axis(labelAngle = 0)),
    alt.Y("unique_hospitals:Q", title = "Number of unique hospitals", scale = alt.Scale(domain = [7000,8000]))
).properties(width = 250, height = 250)
chart_unique
```

    b.
    The numbers of unique hospitals are the same as the numbers of observations for all years we examined. This suggests that this dataset is structured so that each row contains parameters/attributions of each hospital.

## Identify hospital closures in POS file (15 pts) (*)

1. 

```{python}
# Make termination status per year a separate column
df_pos2016_short.rename(
    columns={"PGM_TRMNTN_CD": "PGM_TRMNTN_CD_2016"}, inplace=True)
df_pos2017_short.rename(
    columns={"PGM_TRMNTN_CD": "PGM_TRMNTN_CD_2017"}, inplace=True)
df_pos2018_short.rename(
    columns={"PGM_TRMNTN_CD": "PGM_TRMNTN_CD_2018"}, inplace=True)
df_pos2019_short.rename(
    columns={"PGM_TRMNTN_CD": "PGM_TRMNTN_CD_2019"}, inplace=True)
```

```{python}
# Create new merged df
df_list = [df_pos2017_short, df_pos2018_short, df_pos2019_short]
df_pos_termination = df_pos2016_short

for df in df_list:
    col_name = df.columns[-2]
    df_pos_termination = df_pos_termination.merge(
        df[["PRVDR_NUM", col_name]], on="PRVDR_NUM", how="outer")

df_pos_termination = df_pos_termination.drop(columns=["DATA_YEAR"])
```

```{python}
# Add suspected termination year
def add_termination_year(row):
    if row["PGM_TRMNTN_CD_2016"] != 0:
        return None
    elif row["PGM_TRMNTN_CD_2017"] != 0:
        return 2017
    elif row["PGM_TRMNTN_CD_2018"] != 0:
        return 2018
    elif row["PGM_TRMNTN_CD_2019"] != 0:
        return 2019
    return None


df_pos_termination["TRMNTN_YEAR"] = df_pos_termination.apply(
    add_termination_year, axis=1)
```


```{python}
# Create list of closed hospitals that were active in 2016
df_pos_active2016 = df_pos_termination[df_pos_termination["PGM_TRMNTN_CD_2016"] == 0]

closed_hospitals = []
closed_hospitals = df_pos_active2016["FAC_NAME"][df_pos_termination["TRMNTN_YEAR"] > 2016].tolist()

len(closed_hospitals)
```

174 hospitals

```{python}
# Create df of closed hospitals
# MAY NEED TO CHANGE TO LIST
df_pos_closed = df_pos_active2016.loc[df_pos_active2016["TRMNTN_YEAR"].notna()]
df_pos_closed = df_pos_closed[["FAC_NAME", "ZIP_CD", "TRMNTN_YEAR"]]
```

2. 

```{python}
# Sort by hospital name
df_pos_closed.sort_values(by="FAC_NAME", inplace=True)
df_pos_closed.head(10)
```

3. 
    a.

```{python}
# Count number of active hospitals per year
df_pos_active2016 = df_pos_termination[df_pos_termination["PGM_TRMNTN_CD_2016"] == 0][["PRVDR_NUM", "FAC_NAME", "ZIP_CD"]]

df_suspect_closures = pd.DataFrame()
status_per_year = ["PGM_TRMNTN_CD_2017", "PGM_TRMNTN_CD_2018", "PGM_TRMNTN_CD_2019"]

for year, status in zip([2017, 2018, 2019], status_per_year):
    active = df_pos_termination[df_pos_termination[status] == 0]["PRVDR_NUM"]
    closures = df_pos_active2016[~df_pos_active2016["PRVDR_NUM"].isin(active)]
    closures["TRMNTN_YEAR"] = year

    suspect_closures = pd.concat([df_suspect_closures, closures])

    df_pos_active2016 = df_pos_active2016[df_pos_active2016["PRVDR_NUM"].isin(active)]
```

```{python}
# Count number of active hospitals per year
active_counts = df_pos_termination.groupby("ZIP_CD").agg(
    active_2016=("PGM_TRMNTN_CD_2016", lambda x: (x == 0).sum()),
    active_2017=("PGM_TRMNTN_CD_2017", lambda x: (x == 0).sum()),
    active_2018=("PGM_TRMNTN_CD_2018", lambda x: (x == 0).sum()),
    active_2019=("PGM_TRMNTN_CD_2019", lambda x: (x == 0).sum())
).reset_index()
```

```{python}
#|eval: False
a
```

```{python}
#|eval: False
# SKIP
# Count number of active hospitals per year
df_pos_active = df_pos_termination[df_pos_termination["PGM_TRMNTN_CD_2016"] == 0].groupby(
    "ZIP_CD").size().reset_index(name="active_2017")

for col_index in range(-4, -2, 1):
    col_name = df_pos_termination.columns[col_index]
    col = df_pos_termination[col_name]
    new_col_name = "active_" + str(col_index + 2022)
    active = df_pos_termination[col == 0].groupby(
        "ZIP_CD").size().reset_index()
    active.rename(columns={0: new_col_name}, inplace=True)
    df_pos_active = df_pos_active.merge(
        active[["ZIP_CD", new_col_name]], on="ZIP_CD", how="outer").fillna(0)

# Compare number of active hospitals with closures
df_pos_active["diff_1718"] = df_pos_active["active_2017"] - \
    df_pos_active["active_2018"]
df_pos_active["diff_1819"] = df_pos_active["active_2018"] - \
    df_pos_active["active_2019"]
```

```{python}
#|eval: False
# Add back to original df
combined = df_pos_termination.merge(active_counts, on='ZIP_CD')
```

```{python}
#|eval: False
suspected_closures = combined[(combined["PGM_TRMNTN_CD_2017"].isna() | combined["PGM_TRMNTN_CD_2017"] > 0)]
```

```{python}
#|eval: False
suspected_closures = combined[(combined["PGM_TRMNTN_CD_2017"].isna() | (combined["PGM_TRMNTN_CD_2017"] > 0)) |
                              (combined["PGM_TRMNTN_CD_2018"].isna() | (combined["PGM_TRMNTN_CD_2018"] > 0)) |
                              (combined["PGM_TRMNTN_CD_2019"].isna() | (combined["PGM_TRMNTN_CD_2019"] > 0))]
```

```{python}
#|eval: False
# Filter out suspected closures where active count does not decrease
for year in ["PGM_TRMNTN_CD_2016", "PGM_TRMNTN_CD_2017", "PGM_TRMNTN_CD_2018", "PGM_TRMNTN_CD_2019"]:
    next_year = 'active_' + year[-4:]
    
    # Remove suspected closures where active count stays the same or increases
    test = test[~((test[year].isna() | (test[year] == 1)) & 
                (test[next_year] >= test['active_' + year[-4:]]))]

# Final DataFrame without suspected closures
final_df = test.drop(columns=[col for col in test.columns if col.endswith('_count')])
```

There are three (3) suspect hospital closures that fit the definition of potentially being a merger/acquisition. Two (2) were in 2017 and one (1) was in 2018.

```{python}
#|eval: False
# Fix termination dates to identify suspect hospital closures
def revise_termination_year(row):
    if row["PGM_TRMNTN_CD_2016"] != 0:
        if ((row["PGM_TRMNTN_CD_2017"] == 0) | (row["PGM_TRMNTN_CD_2018"] == 0) | (row["PGM_TRMNTN_CD_2019"] == 0)):
            return 2016
    elif row["PGM_TRMNTN_CD_2017"] != 0:
        return 2017
    elif row["PGM_TRMNTN_CD_2018"] != 0:
        return 2018
    elif row["PGM_TRMNTN_CD_2019"] != 0:
        return 2019
    return None

df_pos_termination["TRMNTN_YEAR"] = df_pos_termination.apply(
    revise_termination_year, axis=1)

# Remove suspect hospital closures
df_pos_termination_revised = df_pos_termination[df_pos_termination["TRMNTN_YEAR"] != 2016]
```

```{python}
#|eval: False
        if np.isnan(row["PGM_TRMNTN_CD_2016"]):
            return None
```

    b.

```{python}
#|eval: False
# Create revised list of closed hospitals
df_pos2016_active_revised = df_pos_termination_revised[df_pos_termination_revised["PGM_TRMNTN_CD_2016"] == 0]
closed_hospitals = []
closed_hospitals = df_pos2016_active_revised["FAC_NAME"][df_pos2016_active_revised["TRMNTN_YEAR"] > 2016].tolist()

len(closed_hospitals)
```

    c.

## Download Census zip code shapefile (10 pt) 

1. 
    a. The file types and the   types of information for each of those included in the data is below;

      - .shp : feature geometrics

      - .shx : positional index

      - .dbf : attribute information

      - .prj : description about the CRS

      - .xml : metadata for the data
      
    b. 

```{python}
# Define a function to get the file size
def get_filesize(path):
    size = os.path.getsize(path)
    if size > (1024 ** 2):
        size = size / (1024 ** 2)
        result = f"{round(size,1)}MB"
    elif size > 1024:
        size = size / 1024
        result = f"{round(size,1)}KB"
    else:
        result = f"{round(size,1)}bytes"
    return result

# Get the file sizes for each file
shape_filetypes = ["shp", "shx", "dbf", "prj", "xml"]

paths_shape = [os.path.join(base, "gz_2010_us_860_00_500k", f"gz_2010_us_860_00_500k.{x}") for x in shape_filetypes]

shape_filesizes = pd.DataFrame(
  {"file": [f"gz_2010_us_860_00_500k.{x}" for x in shape_filetypes],
  "size":[get_filesize(x) for x in paths_shape]
  }
)

print(shape_filesizes)
```

2. 

```{python}
# Load the shp file
path_shp = os.path.join(base, r"gz_2010_us_860_00_500k\gz_2010_us_860_00_500k.shp")
df_shp = gpd.read_file(path_shp)
```

```{python}
# Specify the variable for ZIP code in the dataset
df_shp.head
df_shp["ZCTA5"] = df_shp["ZCTA5"].astype(int)
```

From the metadata in the XML file, we can restrict the zip code by filtering the column "ZCTA5". Now given that Texas zip codes start from 75-79, we can operate the restriction as below;

```{python}
# Restrict dataset to Texas ZIP codes
df_shp_tx = df_shp[(df_shp["ZCTA5"] >= 75000) & (df_shp["ZCTA5"] < 80000)]

# Extract a list of Texas ZIP codes
zip_tx = df_shp_tx["ZCTA5"].value_counts().index.astype(int).to_list()

# Summarize the number of hospitals for each zip code on 2016 subset of POS data
df_pos2016_short_tx = df_pos2016_short[df_pos2016_short["ZIP_CD"].isin(zip_tx)]
summary_pos2016_short_tx = df_pos2016_short_tx.groupby("ZIP_CD").size().reset_index()
summary_pos2016_short_tx.columns = ["ZIP_CD", "HOSPITAL_NUM"]
summary_pos2016_short_tx["ZIP_CD"] =summary_pos2016_short_tx["ZIP_CD"].astype(int)

print(summary_pos2016_short_tx)
```

```{python}
# Merge the dataframes
df_shp_tx_merge = df_shp_tx.merge(
    summary_pos2016_short_tx,
    left_on = "ZCTA5",
    right_on = "ZIP_CD",
    how = "outer"
)

# Replace NAs in the number of hospitals with the value 0
df_shp_tx_merge["HOSPITAL_NUM"] = df_shp_tx_merge["HOSPITAL_NUM"].fillna(0).astype(int).astype("category")

# Plot the number of hospital in TX on the map
df_shp_tx_merge.plot(column = "HOSPITAL_NUM", linewidth = 0.5, edgecolor = "0.8", legend = True, figsize = (8, 8))
plt.axis("off")
plt.title("Number of Hospitals by ZIP code in Texas")
plt.show
```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 

```{python}
# Create a centroid column for all ZIP codes
zips_all_centroids = df_shp.copy()

zips_all_centroids["geometry"] = zips_all_centroids.centroid

print(zips_all_centroids.head())
```

Dimentions:

```{python}
print(zips_all_centroids.shape)
```

33120 rows and 6 columns.

Columns: the columuns in the dataset is below;

```{python}
print(zips_all_centroids.columns)
```

And from the metadata, the meaning for each column is;

- "GEO_ID": A unique identifier for each geographic entity

- "ZCTA5": The 5-digit ZIP Code Tabulation Area code

- "NAME": The name of the geographic entity

- "LSAD": Legal/Statistical Area Description code

- "CENSUSAREA": The area of the entity in square miles

- "geometry": the geometries (polygons) for each area

2. 

```{python}
# Create subset for all zip codes in Texas
zips_texas_centroids = zips_all_centroids[zips_all_centroids["ZCTA5"].isin(zip_tx)]

# Count the unique zip codes in the subset
print(len(zips_texas_centroids["ZCTA5"].unique()))
```

The Texas subset includes 1935 unique zip codes.

From the map in the wikipedia page cited in the PDF, the border states has the prefixes for Zip code of "70", "71", "72", "73", "74", "87", "88". Thus; 

```{python}
# Specify zipcodes for border states
zips_borderstates_centroids = zips_all_centroids[((zips_all_centroids["ZCTA5"] >= 70000) & (zips_all_centroids["ZCTA5"] < 75000)) | ((zips_all_centroids["ZCTA5"] >= 87000) & (zips_all_centroids["ZCTA5"] < 89000))]

# Extract a list of Texas ZIP codes
zip_borderstates = zips_borderstates_centroids["ZCTA5"].value_counts().index.astype(int).to_list()

zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids["ZCTA5"].isin(tuple(zip_tx + zip_borderstates))]

# Count the unique zip codes in the subset
print(len(zips_texas_borderstates_centroids["ZCTA5"].unique()))
```

The Texas and border-states subset includes 4057 unique zip codes.

3. 

```{python}
# Summarize the number of hospitals for each zip code on 2016 subset of POS data
df_pos2016_short_tx_borderstates = df_pos2016_short[df_pos2016_short["ZIP_CD"].isin(zip_tx + zip_borderstates)]
summary_pos2016_short_tx_borderstates = df_pos2016_short_tx_borderstates.groupby("ZIP_CD").size().reset_index()
summary_pos2016_short_tx_borderstates.columns = ["ZIP_CD", "HOSPITAL_NUM"]
summary_pos2016_short_tx_borderstates["ZIP_CD"] =summary_pos2016_short_tx_borderstates["ZIP_CD"].astype(int)

# Merge the dataframes
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    summary_pos2016_short_tx_borderstates,
    left_on = "ZCTA5",
    right_on = "ZIP_CD",
    how = "outer"
)
zips_withhospital_centroids["HOSPITAL_NUM"] = zips_withhospital_centroids["HOSPITAL_NUM"].fillna(0).astype(int)
zips_withhospital_centroids = zips_withhospital_centroids[zips_withhospital_centroids["HOSPITAL_NUM"] != 0]

print(zips_withhospital_centroids.head(5))
```

We merged the GeoDataFrame "zips_texas_borderstates_centroids" with the pos data for short-term hospitals in 2016 merging on Zip codes of the area , filtering to the ZIP codes whose "HOSPITAL_NUM" is not 0 (those which had at least one hospital in 2016).

4. 
    a.

```{python}
# Create a subset of 10 zip codes in TX for testing the calculation of distance
zips_texas_test = zips_texas_centroids.head(10)

# Start timer
start = time.time()

# Join the dataframes
join_test = gpd.sjoin_nearest(
    zips_texas_test,
    zips_withhospital_centroids,
    how = "inner",
    distance_col = "distance"
)

# Find out nearest distance for each TX ZIP code
join_test_result = join_test.loc[join_test.groupby("ZCTA5_left")["distance"].idxmin()]

print("For each ZIP code in Texas ('ZCTA5_left'), the minimum distance to the nearest ZIP code with at least one hospital ('ZCTA5_right') is:")

print(join_test_result[["ZCTA5_left", "ZCTA5_right", "distance"]].head(5))

# Count the lap time
time_count = time.time() - start

# Show the lap time once the row number is correct
print("This time, to join the datasets, it took approximately",
 round(time_count, 2), "seconds.")

time_estimate = (time_count * 1935 / 10)

print(f"As there are 1935 ZIP codes in Texas, we can estimate that it would take approximately {round(time_count, 2)} * 1935 / 10 = {round(time_estimate, 2)} seconds.")
```

    b.

```{python}
# Start timer
start = time.time()

# Join the dataframes
join_tx = gpd.sjoin_nearest(
    zips_texas_centroids,
    zips_withhospital_centroids,
    how = "right",
    distance_col = "distance"
)

# Find out nearest distance for each TX ZIP code
join_tx_min = join_tx.loc[join_tx.groupby("ZCTA5_left")["distance"].idxmin()]

print("For each ZIP code in Texas ('ZCTA5_left'), the minimum distance to the nearest ZIP code with at least one hospital ('ZCTA5_right') is:")

print(join_tx_min[["ZCTA5_left", "ZCTA5_right", "distance"]].head(5))

# Count the lap time
time_count = time.time() - start

# Show the lap time once the row number is correct
print("To join the datasets, it took approximately",
 round(time_count, 2), "seconds, which is the quite same level as the test, against our estimation above.")
```

    c. As the .prj file notes, its unit is a degree (of latitude). A degree of latitude is constantly apploximately convertible to 69 miles, while longitude varies depend on the location, but still maximum about 69 miles.(https://www.thoughtco.com/degree-of-latitude-and-longitude-distance-4070616)

    Thus, when interpret the dataset according to the .prj file, it would be suitable to estimate 1 degree = 69 miles.


5. 
    a. The unit is a degree.

    b. 

```{python}
# Summarize average distance from the nearest hospital (by ZIP code where the hospital locates) for each ZIP code
join_tx_avg = join_tx.groupby("ZCTA5_left")["distance"].agg("mean").reset_index(name = "distance")

# Convert to mile
join_tx_avg["distance"] = join_tx_avg["distance"] * 69

print(join_tx_avg)
```

    c.

```{python}
# Merge the dataframes
zips_texas_centroids_merge = zips_texas_centroids.merge(
    join_tx_avg,
    left_on = "ZCTA5",
    right_on = "ZCTA5_left",
    how = "right"
)

# Plot the number of hospital in TX on the map
zips_texas_centroids_merge.plot(column = "distance", linewidth = 0.5, edgecolor = "0.8", cmap = "OrRd", legend = True, figsize = (8, 8))
plt.axis("off")
plt.title("Average Distance to the Nearest Hospitals Hospitals by ZIP code in Texas(mil))")
plt.show
```

## Effects of closures on access in Texas (15 pts)

1. 

```{python}
# Subset the appended dataset
df_pos_append["ZIP_CD"] = df_pos_append["ZIP_CD"].astype(int)
df_pos_append_tx = df_pos_append[(df_pos_append["ZIP_CD"] >= 75000) & (df_pos_append["ZIP_CD"] < 80000)]

# Summarize the number of directly affected zip codes
summary_direct = df_pos_append_tx[df_pos_append_tx["PGM_TRMNTN_CD"] != 0].groupby(["ZIP_CD", "FAC_NAME"]).size().reset_index()

summary_direct = summary_direct.groupby("ZIP_CD").size().reset_index()
summary_direct.columns = ["ZIP code", "Number of closures"]

print(summary_direct)
```

2. 

```{python}
# Merge the summary to the GeoDataFrame
df_shp_tx_merge = df_shp_tx_merge.merge(
    summary_direct,
    left_on = "ZCTA5",
    right_on = "ZIP code",
    how = "outer"
)

df_shp_tx_merge["DIRECT"] =df_shp_tx_merge["Number of closures"].apply(pd.isna)

# Dissolve the GeoDataFrame by the parameter for directly affected ZIP codes
dissolved = df_shp_tx_merge[["ZCTA5","geometry", "DIRECT"]].dissolve(by = "DIRECT", aggfunc = "sum").reset_index()
dissolved

# Plot a choropleth of directly affected ZIP codes
dissolved.plot(column = "DIRECT", linewidth = 1, edgecolor = "0.8", legend = True, figsize = (8, 8))
plt.axis("off")
plt.title("Directly Affected ZIP codes in Texas")
plt.show
```

```{python}
# Count the number of directly affected ZIP codes
print(len(summary_direct))
```

There are 344 directly affected zip codes in Texas.

3. 

```{python}
# Create a GeoDataFrame of the directly affected zip codes
direct_zips = dissolved[dissolved["DIRECT"] == 1]

# Create a 10-mile buffer around direct_zips
buf_degree = 10 / 69

buffer_10 = direct_zips.copy()
buffer_10["geometry"] = buffer_10.geometry.buffer(buf_degree)

# Spatial join with the overall Texas zip code shapefile
indirect_zips = gpd.sjoin(df_shp_tx, buffer_10, how = "inner", predicate = "intersects")

print(len(indirect_zips))
```

There are 1935 indirectly affected zipcodes in Texas.

4. 

```{python}
fig, ax = plt.subplots()
direct_zips.plot(ax = ax, color = "red", alpha = 0.7, label = "Direct").set_axis_off()
buffer_10.plot(ax = ax, color = "purple", alpha = 0.5, label = "10-mile").set_axis_off()
indirect_zips.plot(ax = ax, color = "blue", alpha = 0.3, label = "Indirect")

plt.show()
```

## Reflecting on the exercise (10 pts) 
(Partner 1: Hiro) 

(Partner 2: Luis)